import collections
import math
import time

import numpy as np
import pandas as pd
from keras.callbacks import ModelCheckpoint
from keras.layers import Embedding, LSTM, Dense
from keras.losses import BinaryCrossentropy
from keras.metrics import BinaryAccuracy
from keras.models import Sequential, load_model
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from sklearn.metrics import classification_report
from sklearn.model_selection import StratifiedShuffleSplit

bytecode_vul_data = np.array([
    # {
    #     'label': 'Authentication through tx.origin.csv',
    #     'value': 1,
    # },
    {
        'label': 'Delegatecall Injection.csv',
        'value': 0,
    },
    {
        'label': 'Frozen Ether.csv',
        'value': 1,
    },
    {
        'label': 'Leaking Ether to arbitrary address.csv',
        'value': 2,
    },
    {
        'label': 'Timestamp dependence.csv',
        'value': 3,
    },
    # {
    #     'label': 'Ugradeable contract.csv',
    #     'value': 5,
    # },
    # {
    #     'label': 'Unprotected Suicide.csv',
    #     'value': 7,
    # },
    # {
    #     'label': 'Outdated Solidity version.csv',
    #     'value': 8,
    # },
])
NUM_CLASS = len(bytecode_vul_data) + 1
NO_VUL_LABEL = len(bytecode_vul_data)
label_dict = np.array([
    {
        'label': 'No vulnerability',
        'value': '4',
    },
    {
        'label': 'Delegate call Injection',
        'value': '0',
    },
    {
        'label': 'Frozen Ether',
        'value': '1',
    },
    {
        'label': 'Leaking Ether to arbitrary address',
        'value': '2',
    },
    {
        'label': 'Timestamp dependence',
        'value': '3',
    },
    # {
    #     'label': 'Upgradeable contract',
    #     'value': '5',
    # }
])

CONTRACT_TYPE_COLUMN = "Contract Type"
NUMBER_OF_RECORD_COLUMN = "Number of records"


def nlp_preprocess(df):
    n_most_common_opcodes = 1000  # 8000
    max_len = 130

    tokenizer = Tokenizer(num_words=n_most_common_opcodes, lower=False)

    tokenizer.fit_on_texts(df['BYTECODE'].values)

    sequences = tokenizer.texts_to_sequences(df['BYTECODE'].values)

    # word_index = tokenizer.word_index
    # print('Found %s unique tokens.' % len(word_index))

    _X = pad_sequences(sequences, maxlen=max_len)

    return _X


def df_to_xy(df, numClass):
    _X = nlp_preprocess(df)
    # _y = to_categorical(df['LABEL'], num_classes=numClass)
    _y = np.array(df['LABEL'])
    return _X, _y


def format_value(df, value):
    df['LABEL'] = value
    df = df[df['BYTECODE'].apply(lambda x: str(type(x)) == "<class 'str'>")]
    return df


def process_data():
    no_vul_contract = 'Contracts_No_Vul.csv'
    _non_vul_data = pd.read_csv('./data/' + no_vul_contract, usecols=['OPCODE', 'CATEGORY'])
    _non_vul_data.columns = ['BYTECODE', 'LABEL']
    _non_vul_data['LABEL'] = NO_VUL_LABEL

    formatData = []
    for data in bytecode_vul_data:
        p = pd.read_csv('./data/' + data['label'], usecols=['BYTECODE', 'LABEL'])
        if int(data['value']) == 0:
            p = p[:35171]
        if int(data['value']) == 1:
            p = p[:97359]
        if int(data['value']) == 2:
            p = p[:595]
        if int(data['value']) == 3:
            p = p[:54666]
        p = format_value(p, data['value'])
        formatData.append(p)

    _vul_data = pd.concat(formatData)

    # concatenate vulnerable and non-vulnerable into one set
    _dataset = pd.concat([_vul_data, _non_vul_data], ignore_index=True)
    print(_dataset['LABEL'].value_counts())
    return _dataset


def build_model_binary(most_common_opcode, input_length):
    model = Sequential()
    model.add(Embedding(most_common_opcode, 128, input_length=input_length))
    model.add(LSTM(128, dropout=0.1, recurrent_dropout=0.1))
    model.add(Dense(1, activation='sigmoid'))
    model.summary()

    model.compile(loss=BinaryCrossentropy(), optimizer=RMSprop(), metrics=[BinaryAccuracy()])
    return model


def build_multiclass_model(max_features, maxlen):
    """Build multiclass LSTM model for multiclass classification"""
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length=maxlen))
    model.add(LSTM(128, dropout=0.1, recurrent_dropout=0.1))
    model.add(Dense(NUM_CLASS, activation='softmax'))

    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

    return model


def create_class_weight(labels_dict, mu):
    """Create weight based on the number of domain name in the dataset"""
    total = np.sum(list(labels_dict.values()))
    keys = labels_dict.keys()
    class_weight = dict()

    for key in keys:
        score = math.pow(total / float(labels_dict[key]), mu)
        class_weight[key] = score

    return class_weight


def classification_report_csv(_y_test_format, _y_pred, report_name, _label_dict=label_dict):
    _report = classification_report(_y_test_format, _y_pred, output_dict=True)
    df_classification_report = pd.DataFrame(_report).transpose()
    indexes = df_classification_report.index.values

    j = 0
    for _label in _label_dict:
        if _label == indexes[j]:
            indexes[j] = _label['label']
            j += 1

    df_classification_report.insert(0, CONTRACT_TYPE_COLUMN, indexes)
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    df_classification_report.to_csv('./report/' + report_name + timestamp + '.csv', index=False)


def run(max_epoch=5, n_folds=10, batch_size=128):
    # split processed dataset into training and test
    dataset = process_data()  # read data
    # preprocessing data and one-hot code
    X, y_multi = df_to_xy(dataset, len(bytecode_vul_data) + 1)  # y_multi: multilabel with no-vul and vul
    # binary label
    y_binary = y_multi.copy()
    y_binary = np.where(y_binary == NO_VUL_LABEL, 0, 1)

    sss = StratifiedShuffleSplit(n_folds, test_size=0.2, random_state=0)
    fold = 0
    for train, test in sss.split(X, y_binary, y_multi):
        fold = fold + 1
        print("=============FOLD %d================" % fold)
        # X_train, y_train use for evaluating binary model, X_test, y_test for testing binary model
        X_train, X_test, y_train, y_test, y_mul_train, y_mul_test = \
            X[train], X[test], y_binary[train], y_binary[test], y_multi[train], y_multi[test]

        # X_vul, y_vul use for multiclass classification
        y_vul = []
        X_vul = []
        # create the multilabel data train by removing the no-vul data
        for i in range(len(y_mul_train)):
            if y_mul_train[i] != NO_VUL_LABEL:
                X_vul.append(X_train[i])
                y_vul.append(y_mul_train[i])

        X_vul = np.array(X_vul)
        y_vul = np.array(y_vul)

        # Build the model for binary classification
        model_binary = build_model_binary(1000, X.shape[1])

        # callback to save model
        checkpoint_binary_filepath = 'best_model_binary.hdf5'
        model_checkpoint_callback = ModelCheckpoint(
            filepath=checkpoint_binary_filepath,
            monitor='val_loss',
            mode='min',
            save_best_only=True)

        # Train model
        model_binary.fit(X_train, y_train, batch_size=batch_size, epochs=max_epoch,
                         callbacks=[model_checkpoint_callback], validation_split=0.05)
        # Multiclass classification
        # Split data
        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
        for train_multi, test_multi in sss2.split(X_vul, y_vul):
            X_train_multi, X_holdout_multi, y_train_multi, y_holdout_multi = X_vul[train_multi], X_vul[test_multi], \
                                                                             y_vul[train_multi], y_vul[test_multi]
        labels_dict = collections.Counter(y_train_multi)
        # Create class weight
        class_weight = create_class_weight(labels_dict=labels_dict, mu=0.3)
        # Build model
        model_multi = build_multiclass_model(1000, X.shape[1])
        # Callback to save the best model
        checkpoint_multi_filepath = 'best_model_multi.hdf5'
        model_checkpoint_callback = ModelCheckpoint(
            filepath=checkpoint_multi_filepath,
            monitor='val_loss',
            mode='min',
            save_best_only=True)
        # Train model
        model_multi.fit(X_train_multi, y_train_multi, batch_size=batch_size, epochs=max_epoch,
                        class_weight=class_weight,
                        callbacks=[model_checkpoint_callback], validation_data=(X_holdout_multi, y_holdout_multi))

        # load the best model
        best_model_binary = load_model(checkpoint_binary_filepath)
        best_model_multi = load_model(checkpoint_multi_filepath)

        # calculate confusion matrix and combine to multilabel
        y_pred_binary = best_model_binary.predict(X_test)
        y_pred_binary = y_pred_binary.ravel()
        y_pred_binary = [1 if (y_pred_binary[x] > 0.5) else NO_VUL_LABEL for x in range(len(y_pred_binary))]
        y_pred_binary = np.array(y_pred_binary)

        X_dga_test = []

        for i in range(len(y_pred_binary)):
            if y_pred_binary[i] == 1:
                X_dga_test.append(X_test[i])

        X_dga_test = np.array(X_dga_test)

        y_pred_multi = best_model_multi.predict(X_dga_test)
        y_pred_multi = np.argmax(y_pred_multi, axis=1)

        # combine by replacing with compatible label
        j = 0
        for i in range(len(y_pred_binary)):
            if y_pred_binary[i] == 1:
                y_pred_binary[i] = y_pred_multi[j]
                j = j + 1

        print(classification_report(y_pred_binary, y_mul_test))
        # save report to csv
        classification_report_csv(y_pred_binary, y_mul_test)

        print("=============END FOLD %d=============" % fold)


if __name__ == "__main__":
    run()
